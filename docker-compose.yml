services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"   # optional expose; internal is enough for api
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 300s
      timeout: 300s
      retries: 50

  # One-shot job to ensure model is present before api starts
  ollama-pull:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh","-c"]
    command: ["ollama pull ${OLLAMA_MODEL}"]
    restart: "no"

  api:
    build:
      context: .
      dockerfile: backend/Dockerfile
    environment:
      FLASK_ENV: ${FLASK_ENV}
      SECRET_KEY: ${SECRET_KEY}
      DATABASE_URL: ${DATABASE_URL}
      OLLAMA_URL: ${OLLAMA_URL}
      OLLAMA_MODEL: ${OLLAMA_MODEL}
    depends_on: []
      # ollama:
        # condition: service_healthy
      # ollama-pull:
        # condition: service_completed_successfully
    volumes:
      - appdata:/data
    expose:
      - "5000"
    restart: unless-stopped

  web:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    depends_on:
      - api
    expose:
      - "80"
    restart: unless-stopped

  proxy:
    image: caddy:latest
    depends_on:
      - web
      - api
    ports:
      - "80:80"
      # For HTTPS later: - "443:443"
    volumes:
      - ./proxy/Caddyfile:/etc/caddy/Caddyfile
    restart: unless-stopped

volumes:
  ollama:
  appdata:
