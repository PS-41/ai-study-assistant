services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"   # optional expose; internal is enough for api
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 20

  # One-shot job to ensure model is present before api starts
  ollama-pull:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh","-c"]
    command: ["ollama list | grep -q ${OLLAMA_MODEL} || ollama pull ${OLLAMA_MODEL}"]
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    restart: "no"

  api:
    build:
      context: .
      dockerfile: backend/Dockerfile
    environment:
      FLASK_ENV: ${FLASK_ENV}
      SECRET_KEY: ${SECRET_KEY}
      DATABASE_URL: ${DATABASE_URL}
      LLM_PROVIDER: ${LLM_PROVIDER}
      OLLAMA_URL: ${OLLAMA_URL}
      OLLAMA_MODEL: ${OLLAMA_MODEL}
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY}
      OPENROUTER_MODEL: ${OPENROUTER_MODEL}
    depends_on:
      ollama:
        condition: service_healthy
      ollama-pull:
        condition: service_completed_successfully
    volumes:
      - appdata:/app/data
    expose:
      - "5000"
    restart: unless-stopped

  web:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    depends_on:
      - api
    expose:
      - "80"
    restart: unless-stopped

  proxy:
    image: caddy:latest
    depends_on:
      - web
      - api
    ports:
      - "80:80"
      # For HTTPS later: - "443:443"
    volumes:
      - ./proxy/Caddyfile:/etc/caddy/Caddyfile
    restart: unless-stopped

volumes:
  ollama:
  appdata:
